{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.effects\n",
    "import colorednoise as cn\n",
    "from pydub import AudioSegment, effects\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import fastai.torch_core\n",
    "from fastai.vision.all import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "import shutil\n",
    "import imageio\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or Kaggle, which makes some difference for the code below.\n",
    "\n",
    "DATA_DIR = Path(os.getcwd())\n",
    "try:\n",
    "    import google.colab\n",
    "    colab=True\n",
    "    \n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    DATA_DIR = Path('/content/gdrive/MyDrive/DAT255')\n",
    "except:\n",
    "    DATA_DIR = DATA_DIR / '..' / 'data'\n",
    "    colab=False\n",
    "    \n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUB_DIRS = ['external','interim','processed','raw']    \n",
    "\n",
    "for d in SUB_DIRS:\n",
    "    dir_path = DATA_DIR / d\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "DATA_DIR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = fastai.torch_core.default_device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = fastai.torch_core.default_device('mps')\n",
    "else:\n",
    "    device = fastai.torch_core.default_device('cpu')\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:53:26.919854Z",
     "start_time": "2024-04-15T20:53:26.912629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FSDKaggle2018.audio_test', 'FSDKaggle2018.audio_train', 'FSDKaggle2018.meta']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['FSDKaggle2018.meta', 'FSDKaggle2018.audio_train', 'FSDKaggle2018.audio_test']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_and_unzip(url, dir_name):\n",
    "    if not os.path.isdir(dir_name / url.rsplit('/')[-1].rsplit('.', 1)[0]):\n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        zip_file_name = f\"{dir_name}.zip\"\n",
    "        with open(zip_file_name, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Unzip the file\n",
    "        with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dir_name)\n",
    "\n",
    "        # Remove the zip file\n",
    "        os.remove(zip_file_name)\n",
    "\n",
    "DOWNLOAD_PATH = DATA_DIR / 'external' / 'fsdkaggle2018'\n",
    "DOWNLOAD_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download_and_unzip('https://zenodo.org/records/2552860/files/FSDKaggle2018.audio_test.zip', DOWNLOAD_PATH)\n",
    "download_and_unzip('https://zenodo.org/records/2552860/files/FSDKaggle2018.audio_train.zip', DOWNLOAD_PATH)\n",
    "download_and_unzip('https://zenodo.org/records/2552860/files/FSDKaggle2018.meta.zip', DOWNLOAD_PATH)\n",
    "\n",
    "# List the current directory contents\n",
    "print(os.listdir(DOWNLOAD_PATH))\n",
    "\n",
    "['FSDKaggle2018.meta', 'FSDKaggle2018.audio_train', 'FSDKaggle2018.audio_test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(DOWNLOAD_PATH / 'FSDKaggle2018.meta' / 'train_post_competition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for the path to the audio file\n",
    "meta['path'] = meta['fname'].apply(lambda x: os.path.join(DOWNLOAD_PATH, 'FSDKaggle2018.audio_train', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets sort out non-musical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first print all the labels in the dataset\n",
    "labels = meta['label'].unique()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# musical labels\n",
    "musical_labels = ['Hi-hat', 'Saxophone','Trumpet', 'Glockenspiel', 'Cello','Clarinet','Snare_drum','Oboe', 'Flute','Chime', 'Bass_drum','Harmonica', 'Gong','Double_bass','Tambourine', 'Cowbell', 'Electric_piano','Acoustic_guitar', 'Violin_or_fiddle',\n",
    " 'Finger_snapping' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can filter the dataset to only include the musical labels\n",
    "musical_meta = meta[meta['label'].isin(musical_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mixing audio clips\n",
    "\n",
    "- Experimenting with mixing audio clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_audio_clips(clip_paths, output_path, sr=44100):\n",
    "    # Load the first clip\n",
    "    mixed_clip, _ = librosa.load(clip_paths[0], sr=sr)\n",
    "    \n",
    "    # Load and mix each subsequent clip\n",
    "    for clip_path in clip_paths[1:]:\n",
    "        clip, _ = librosa.load(clip_path, sr=sr)\n",
    "        # Ensure the clips are of the same length\n",
    "        min_len = min(len(mixed_clip), len(clip))\n",
    "        mixed_clip = mixed_clip[:min_len] + clip[:min_len]\n",
    "        \n",
    "    # Normalize the mixed clip to prevent clipping\n",
    "    mixed_clip = mixed_clip / np.max(np.abs(mixed_clip))\n",
    "    \n",
    "    # Save the mixed clip to an output file\n",
    "    sf.write(output_path, mixed_clip, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "path1 = DOWNLOAD_PATH / \"FSDKaggle2018.audio_train\" / \"0a0a8d4c.wav\"\n",
    "path2 = DOWNLOAD_PATH / \"FSDKaggle2018.audio_train\" / \"0a2a5c05.wav\"\n",
    "\n",
    "processed_path = DATA_DIR / 'processed'\n",
    "\n",
    "# Create mixed clips directory if it doesn't exist\n",
    "mixed_clips_path = os.path.join(processed_path, 'mixed_clips')\n",
    "if not os.path.exists(mixed_clips_path):\n",
    "    os.makedirs(mixed_clips_path)\n",
    "\n",
    "mix_audio_clips([path1, path2], os.path.join(mixed_clips_path, 'mixed_clip.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mixed clip and the original clips to compare\n",
    "ipd.Audio(path1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(os.path.join(mixed_clips_path, 'mixed_clip.wav'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works fine :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency determination\n",
    "\n",
    "- Want to categorize the clips into frequencies.\n",
    "- Avoid muddy mixes with clips in same frequency range.\n",
    "- Mimick realistic audio mixing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_frequency_range(audio_path, sr=44100):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "    \n",
    "    # Compute the short-time Fourier transform (STFT)\n",
    "    D = np.abs(librosa.stft(y))\n",
    "    \n",
    "    # Define the frequency ranges\n",
    "    ranges = {\n",
    "        'sub_bass': (20, 60),\n",
    "        'bass': (60, 250),\n",
    "        'low_midrange': (250, 500),\n",
    "        'midrange': (500, 2000),\n",
    "        'upper_midrange': (2000, 4000),\n",
    "        'presence': (4000, 6000),\n",
    "        'brilliance': (6000, 20000),\n",
    "    }\n",
    "    \n",
    "    # Compute the spectral energy in each range\n",
    "    energy = {name: D[int(low / sr * D.shape[0]):int(high / sr * D.shape[0])].sum()\n",
    "              for name, (low, high) in ranges.items()}\n",
    "    \n",
    "    # Determine the dominant frequency range\n",
    "    return max(energy, key=energy.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "# Two random audio clips\n",
    "path1 = musical_meta.sample(1).iloc[0]['path']\n",
    "path2 = musical_meta.sample(1).iloc[0]['path']\n",
    "\n",
    "# Create a DataFrame with the paths of the two audio clips\n",
    "df = pd.DataFrame({'path': [path1, path2]})\n",
    "\n",
    "# Determine the frequency range for each clip and add it to the DataFrame\n",
    "df['frequency_range'] = df['path'].apply(determine_frequency_range)\n",
    "\n",
    "print(path1, \"frequency range: \" ,determine_frequency_range(path1), \"instrument: \", musical_meta[musical_meta['path'] == path1]['label'].values[0])\n",
    "print(path2, \"frequency range: \" , determine_frequency_range(path2), \"instrument: \", musical_meta[musical_meta['path'] == path2]['label'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually works\n",
    "\n",
    "- Now, let's add a range to all the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the frequency range for all clips in the dataset\n",
    "musical_meta['frequency_range'] = musical_meta['path'].apply(determine_frequency_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musical_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order of the frequency ranges from low to high\n",
    "order = ['sub_bass', 'bass', 'low_midrange', 'midrange', 'upper_midrange', 'presence', 'brilliance']\n",
    "\n",
    "# Plot the distribution of frequency ranges\n",
    "plt.figure(figsize=(10, 6))\n",
    "musical_meta['frequency_range'].value_counts().loc[order].plot(kind='bar')\n",
    "plt.title('Distribution of Frequency Ranges')\n",
    "plt.xlabel('Frequency Range')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixing clips from different ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental genre-specific adjustments. Might not work as intended. The idea is to apply different effects to the audio clips based on the genre.\n",
    "genre_instruments = {\n",
    "    'jazz': ['Saxophone', 'Trumpet', 'Double_bass', 'Snare_drum', 'Hi-hat', 'Clarinet', 'Trombone', 'Electric_piano'],\n",
    "    'classical': ['Violin_or_fiddle', 'Cello', 'Flute', 'Oboe', 'Clarinet', 'Glockenspiel', 'Chime'],\n",
    "    'rock': ['Electric_piano', 'Acoustic_guitar', 'Snare_drum', 'Bass_drum', 'Hi-hat', 'Tambourine', 'Cowbell'],\n",
    "    'blues': ['Saxophone', 'Trumpet', 'Harmonica', 'Electric_piano', 'Acoustic_guitar', 'Snare_drum', 'Hi-hat'],\n",
    "    'folk': ['Acoustic_guitar', 'Violin_or_fiddle', 'Harmonica', 'Double_bass', 'Flute', 'Tambourine', 'Glockenspiel'],\n",
    "    'electronic': ['Snare_drum', 'Bass_drum', 'Hi-hat', 'Electric_piano', 'Gong', 'Tambourine', 'Finger_snapping', 'Chime'],\n",
    "    'world': ['Harmonica', 'Gong', 'Tambourine', 'Cowbell', 'Oboe', 'Chime', 'Flute', 'Double_bass'],\n",
    "    'wildcard': ['Hi-hat', 'Saxophone', 'Trumpet', 'Glockenspiel', 'Cello', 'Clarinet', 'Snare_drum', 'Oboe', 'Flute', 'Chime', 'Bass_drum', 'Harmonica', 'Gong', 'Double_bass', 'Tambourine', 'Cowbell', 'Electric_piano', 'Acoustic_guitar', 'Violin_or_fiddle', 'Finger_snapping']\n",
    "}\n",
    "\n",
    "# Map instruments to more general categories\n",
    "instrument_map = {\n",
    "    'Snare_drum': 'Snare_drum', 'Bass_drum': 'Bass_drum', 'Hi-hat': 'Hi-hat',\n",
    "    'Tambourine': 'Tambourine', 'Gong': 'Percussion', 'Cowbell': 'Percussion',\n",
    "    'Violin_or_fiddle': 'Strings', 'Cello': 'Strings', 'Double_bass': 'Bass',\n",
    "    'Acoustic_guitar': 'Guitar', 'Electric_piano': 'Keys', 'Flute': 'Winds',\n",
    "    'Clarinet': 'Winds', 'Oboe': 'Winds', 'Saxophone': 'Winds', \n",
    "    'Trumpet': 'Brass', 'Trombone': 'Brass', 'Harmonica': 'Miscellaneous/Other',\n",
    "    'Glockenspiel': 'Glockenspiel', 'Chime': 'Miscellaneous/Other',\n",
    "    'Electric_piano': 'Keys', 'Synthesizers': 'Synthesizers', 'Drum_machine': 'Drums',\n",
    "    'Finger_snapping': 'Percussion'\n",
    "}\n",
    "\n",
    "# Function to map labels in your DataFrame\n",
    "def map_instrument_labels(row):\n",
    "    labels = row['label'].split(', ')\n",
    "    generalized_labels = set(instrument_map[label] for label in labels if label in instrument_map)\n",
    "    return ', '.join(generalized_labels)\n",
    "\n",
    "# Applying the mapping to your DataFrame\n",
    "musical_meta['generalized_label'] = musical_meta.apply(map_instrument_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "      <th>manually_verified</th>\n",
       "      <th>freesound_id</th>\n",
       "      <th>license</th>\n",
       "      <th>path</th>\n",
       "      <th>frequency_range</th>\n",
       "      <th>generalized_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00044347.wav</td>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>0</td>\n",
       "      <td>28739</td>\n",
       "      <td>Attribution</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/00044347.wav</td>\n",
       "      <td>brilliance</td>\n",
       "      <td>Hi-hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001ca53d.wav</td>\n",
       "      <td>Saxophone</td>\n",
       "      <td>1</td>\n",
       "      <td>358827</td>\n",
       "      <td>Attribution</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/001ca53d.wav</td>\n",
       "      <td>midrange</td>\n",
       "      <td>Winds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002d256b.wav</td>\n",
       "      <td>Trumpet</td>\n",
       "      <td>0</td>\n",
       "      <td>10897</td>\n",
       "      <td>Creative Commons 0</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/002d256b.wav</td>\n",
       "      <td>midrange</td>\n",
       "      <td>Brass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0033e230.wav</td>\n",
       "      <td>Glockenspiel</td>\n",
       "      <td>1</td>\n",
       "      <td>325017</td>\n",
       "      <td>Attribution</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/0033e230.wav</td>\n",
       "      <td>upper_midrange</td>\n",
       "      <td>Glockenspiel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00353774.wav</td>\n",
       "      <td>Cello</td>\n",
       "      <td>1</td>\n",
       "      <td>195688</td>\n",
       "      <td>Attribution</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/00353774.wav</td>\n",
       "      <td>midrange</td>\n",
       "      <td>Strings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fname         label  manually_verified  freesound_id  \\\n",
       "0  00044347.wav        Hi-hat                  0         28739   \n",
       "1  001ca53d.wav     Saxophone                  1        358827   \n",
       "2  002d256b.wav       Trumpet                  0         10897   \n",
       "3  0033e230.wav  Glockenspiel                  1        325017   \n",
       "4  00353774.wav         Cello                  1        195688   \n",
       "\n",
       "              license  \\\n",
       "0         Attribution   \n",
       "1         Attribution   \n",
       "2  Creative Commons 0   \n",
       "3         Attribution   \n",
       "4         Attribution   \n",
       "\n",
       "                                                                                                                       path  \\\n",
       "0  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/00044347.wav   \n",
       "1  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/001ca53d.wav   \n",
       "2  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/002d256b.wav   \n",
       "3  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/0033e230.wav   \n",
       "4  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/external/fsdkaggle2018/FSDKaggle2018.audio_train/00353774.wav   \n",
       "\n",
       "  frequency_range generalized_label  \n",
       "0      brilliance            Hi-hat  \n",
       "1        midrange             Winds  \n",
       "2        midrange             Brass  \n",
       "3  upper_midrange      Glockenspiel  \n",
       "4        midrange           Strings  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "musical_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_for_genre(clip, genre, sr=44100):\n",
    "    if genre == 'classical':\n",
    "        # Increase dynamic range; classical music often has wide dynamic swings\n",
    "        clip = clip * (1 + np.var(clip))\n",
    "    elif genre == 'rock':\n",
    "        # Apply compression to decrease dynamic range; rock music often has a compressed, upfront sound\n",
    "        clip = librosa.effects.preemphasis(clip)\n",
    "    elif genre == 'jazz':\n",
    "        # Slightly increase dynamic range and add a subtle reverb to emulate live jazz environments\n",
    "        clip = clip * (1 + 0.5 * np.var(clip))\n",
    "        clip = librosa.effects.preemphasis(clip, coef=0.97)  # Slight pre-emphasis\n",
    "    elif genre == 'blues':\n",
    "        # Apply mild compression and a warmer tone by reducing high frequencies\n",
    "        clip = librosa.effects.preemphasis(clip, coef=0.95)\n",
    "    elif genre == 'folk':\n",
    "        # Enhance the natural dynamics and apply a gentle high-pass filter to emulate acoustic settings\n",
    "        clip = librosa.effects.hpss(clip)[1]\n",
    "    elif genre == 'electronic':\n",
    "        # Normalize to ensure uniform loudness levels typical of electronic music\n",
    "        clip = librosa.util.normalize(clip)\n",
    "    elif genre == 'world':\n",
    "        # Apply a slight increase in dynamic range and a moderate reverb to reflect diverse instrumentation and spaces\n",
    "        clip = clip * (1 + 0.3 * np.var(clip))\n",
    "    elif genre == 'wildcard':\n",
    "        # For wildcard, randomly choose to apply one of the effects mildly to not bias the genre\n",
    "        effects = [lambda x: x, lambda x: librosa.effects.preemphasis(x, coef=0.98), lambda x: x * (1 + 0.2 * np.var(x)), librosa.util.normalize]\n",
    "        clip = random.choice(effects)(clip)\n",
    "\n",
    "    return clip\n",
    "\n",
    "\n",
    "def add_noise(clip, noise_type='white', snr=20):\n",
    "    if noise_type == 'white':\n",
    "        # Generate white noise\n",
    "        noise = np.random.normal(0, 1, len(clip))\n",
    "    elif noise_type == 'pink':\n",
    "        # Generate pink noise using colorednoise\n",
    "        # The exponent for pink noise is 1, beta = 1\n",
    "        noise = cn.powerlaw_psd_gaussian(1, len(clip))\n",
    "    elif noise_type == 'brownian':\n",
    "        # Generate brownian noise using colorednoise\n",
    "        # The exponent for brownian noise is 2, beta = 2\n",
    "        noise = cn.powerlaw_psd_gaussian(2, len(clip))\n",
    "    else:\n",
    "        # TODO: Load custom noise file. For now do nothing\n",
    "        return clip\n",
    "    \n",
    "    # Calculate signal and noise power, then scale noise to achieve the desired SNR\n",
    "    sig_power = np.sum(clip ** 2) / len(clip)\n",
    "    noise_power = np.sum(noise ** 2) / len(noise)\n",
    "    scale = (sig_power / noise_power) / (10 ** (snr / 10))\n",
    "    noise = noise * np.sqrt(scale)\n",
    "    \n",
    "    return clip + noise\n",
    "\n",
    "def random_slice_reassemble(audio_segment, num_slices=4):\n",
    "    slice_length = len(audio_segment) // num_slices\n",
    "    slices = [audio_segment[i * slice_length:(i + 1) * slice_length] for i in range(num_slices)]\n",
    "    random.shuffle(slices)\n",
    "    return sum(slices)\n",
    "\n",
    "def vary_speed(clip, sr, min_speed=0.9, max_speed=1.1):\n",
    "    speed_factor = random.uniform(min_speed, max_speed)\n",
    "    return librosa.effects.time_stretch(clip, rate=speed_factor)\n",
    "\n",
    "def mix_clips_from_different_ranges(df, genre_instruments, output_file_name, sr=44100, clip_length=3, min_groups=3, max_groups=8):\n",
    "    # Randomly select a genre\n",
    "    genre = random.choice(list(genre_instruments.keys()))\n",
    "    instruments = genre_instruments[genre]\n",
    "\n",
    "    # Filter df for the selected instruments\n",
    "    df_genre = df[df['label'].isin(instruments)]\n",
    "\n",
    "    grouped = df_genre.groupby('frequency_range')  # Group the clips by frequency range\n",
    "    num_groups = random.randint(min_groups, max_groups)\n",
    "    selected_groups = random.sample(list(grouped.groups), min(num_groups, len(grouped.groups)))\n",
    "\n",
    "    selected_clips = []\n",
    "    used_instruments = set()\n",
    "    for group in selected_groups:\n",
    "        group_df = grouped.get_group(group)\n",
    "        group_df = group_df[~group_df['label'].isin(used_instruments)]  # Exclude used instruments\n",
    "        if not group_df.empty:\n",
    "            selected_clip = group_df.sample(1).iloc[0]\n",
    "            selected_clips.append(selected_clip)\n",
    "            used_instruments.add(selected_clip['label'])\n",
    "\n",
    "    mixed_clip = np.zeros(int(sr * clip_length))\n",
    "\n",
    "    for row in selected_clips:\n",
    "        clip, _ = librosa.load(row['path'], sr=sr, duration=clip_length)\n",
    "        \n",
    "        clip = adjust_for_genre(clip, genre)  # Add genre-specific adjustments\n",
    "        \n",
    "        # Randomly vary the speed of the clip\n",
    "        if random.random() < 0.5:\n",
    "            clip = vary_speed(clip, sr=sr)\n",
    "        \n",
    "        #Randomly slice and reassemble the clip\n",
    "        if random.random() < 0.3:\n",
    "            clip = random_slice_reassemble(clip)\n",
    "        \n",
    "        # Randomly add noise\n",
    "        if random.random() < 0.5:  # Chance of adding noise\n",
    "            noise_types = ['white', 'pink', 'custom', 'brownian']\n",
    "            noise_type = random.choice(noise_types)  # Randomly select the type of noise to add\n",
    "            clip = add_noise(clip, noise_type=noise_type)\n",
    "        \n",
    "        if len(clip) < len(mixed_clip):\n",
    "            clip = np.tile(clip, int(np.ceil(len(mixed_clip) / len(clip))))[:len(mixed_clip)]\n",
    "        mixed_clip += clip[:len(mixed_clip)]\n",
    "        \n",
    "    mixed_clip = mixed_clip / np.max(np.abs(mixed_clip))\n",
    "    output_path = f\"{output_file_name}.wav\"\n",
    "    sf.write(output_path, mixed_clip, sr)\n",
    "\n",
    "    return [row['label'] for row in selected_clips], genre, output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example mixed clip\n",
    "output_path = os.path.join(mixed_clips_path, 'mixed_clip_from_different_ranges')\n",
    "mix_clips_from_different_ranges(musical_meta, genre_instruments, output_path, sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_directory(folder_path):\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"The directory {folder_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Loop through all items in the directory\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # If it's a file or symlink, delete it\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            # If it's a directory, delete it and all its contents\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "def generate_mixed_clips(df, genre_instruments, output_folder, n_clips, sr=44100, clip_length=3):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    elif os.listdir(output_folder):\n",
    "        clear_directory(output_folder)\n",
    "\n",
    "    mixed_clips_info = []\n",
    "\n",
    "    for i in range(n_clips):\n",
    "        labels, genre, output_path = mix_clips_from_different_ranges(df, genre_instruments, f\"{output_folder}/mixed_clip_{i}\", sr, clip_length)\n",
    "        if labels and output_path:\n",
    "            mixed_clips_info.append({'path': output_path, 'labels': ', '.join(labels), 'genre': genre})\n",
    "\n",
    "    return pd.DataFrame(mixed_clips_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "output_folder = mixed_clips_path\n",
    "n_clips = 5  # Number of mixed clips to generate\n",
    "mixed_clips_df = generate_mixed_clips(musical_meta, genre_instruments, output_folder, n_clips)\n",
    "\n",
    "print(mixed_clips_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spectrograms(df, output_dir, fixed_length_seconds=3):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    clear_directory(output_dir)\n",
    "    \n",
    "    # Sample rate\n",
    "    sr = 44100  \n",
    "    # Calculate fixed length in samples\n",
    "    fixed_length_samples = int(fixed_length_seconds * sr)\n",
    "    \n",
    "    # Initialize an empty list to store spectrogram paths\n",
    "    spectrogram_paths = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(row['path'], sr=sr, mono=True)\n",
    "        \n",
    "        if len(y) < fixed_length_samples:\n",
    "            # Calculate the amount of silence needed\n",
    "            padding_needed = fixed_length_samples - len(y)\n",
    "            # Generate a random offset for the silence padding\n",
    "            offset = np.random.randint(0, padding_needed)\n",
    "            \n",
    "            # Pad the audio signal with silence before and after based on the random offset\n",
    "            silence_before = np.zeros(offset)\n",
    "            silence_after = np.zeros(padding_needed - offset)\n",
    "            y_padded = np.concatenate((silence_before, y, silence_after))\n",
    "        else:\n",
    "            # If the audio is longer than the fixed length, truncate it\n",
    "            y_padded = y[:fixed_length_samples]\n",
    "        \n",
    "        # Generate the spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y_padded, sr=sr, n_mels=128, fmax=22000)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalize the spectrogram\n",
    "        norm_log_mel_spec = (log_mel_spec - log_mel_spec.min()) / (log_mel_spec.max() - log_mel_spec.min())\n",
    "        \n",
    "        # Apply a colormap\n",
    "        colored_spec = cm.viridis(norm_log_mel_spec)\n",
    "        \n",
    "        # Convert to RGB\n",
    "        colored_spec_rgb = (colored_spec[..., :3] * 255).astype(np.uint8)\n",
    "\n",
    "        # Define the file name and save path\n",
    "        file_name = os.path.basename(row['path']).replace('.wav', '_spectrogram.png')\n",
    "        save_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        # Save the spectrogram image\n",
    "        imageio.imwrite(save_path, colored_spec_rgb)\n",
    "        \n",
    "        # Append the save path to the list\n",
    "        spectrogram_paths.append(save_path)\n",
    "    \n",
    "    # Add the list as a new column to the DataFrame\n",
    "    df['spectrogram_path'] = spectrogram_paths\n",
    "\n",
    "    return df  # Return the updated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_clips_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_spectrograms(mixed_clips_df, os.path.join(processed_path, 'spectrograms'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>spectrogram_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_0.wav</td>\n",
       "      <td>Guitar, Bass, Winds, Glockenspiel, Miscellaneous/Other, Strings</td>\n",
       "      <td>folk</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_0_spectrogram.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_1.wav</td>\n",
       "      <td>Hi-hat, Guitar, Brass, Snare_drum, Miscellaneous/Other, Winds, Keys</td>\n",
       "      <td>blues</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_1_spectrogram.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_2.wav</td>\n",
       "      <td>Miscellaneous/Other, Winds, Guitar</td>\n",
       "      <td>folk</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_2_spectrogram.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_3.wav</td>\n",
       "      <td>Winds, Strings, Glockenspiel, Miscellaneous/Other</td>\n",
       "      <td>classical</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_3_spectrogram.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_4.wav</td>\n",
       "      <td>Winds, Guitar, Hi-hat, Miscellaneous/Other, Brass, Keys</td>\n",
       "      <td>blues</td>\n",
       "      <td>/home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_4_spectrogram.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                path  \\\n",
       "0  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_0.wav   \n",
       "1  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_1.wav   \n",
       "2  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_2.wav   \n",
       "3  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_3.wav   \n",
       "4  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/mixed_clips/mixed_clip_4.wav   \n",
       "\n",
       "                                                                labels  \\\n",
       "0      Guitar, Bass, Winds, Glockenspiel, Miscellaneous/Other, Strings   \n",
       "1  Hi-hat, Guitar, Brass, Snare_drum, Miscellaneous/Other, Winds, Keys   \n",
       "2                                   Miscellaneous/Other, Winds, Guitar   \n",
       "3                    Winds, Strings, Glockenspiel, Miscellaneous/Other   \n",
       "4              Winds, Guitar, Hi-hat, Miscellaneous/Other, Brass, Keys   \n",
       "\n",
       "       genre  \\\n",
       "0       folk   \n",
       "1      blues   \n",
       "2       folk   \n",
       "3  classical   \n",
       "4      blues   \n",
       "\n",
       "                                                                                                 spectrogram_path  \n",
       "0  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_0_spectrogram.png  \n",
       "1  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_1_spectrogram.png  \n",
       "2  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_2_spectrogram.png  \n",
       "3  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_3_spectrogram.png  \n",
       "4  /home/bjorn/git/dat255-audio_project-g11/notebooks/../data/processed/spectrograms/mixed_clip_4_spectrogram.png  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_clips_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:48:53.167689Z",
     "start_time": "2024-04-15T20:43:25.253323Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, let's generate a bunch of mixed clips and spectrograms\n",
    "n_clips = 10000\n",
    "output_folder = os.path.join(processed_path, 'mixed_clips')\n",
    "mixed_clips_df = generate_mixed_clips(musical_meta, genre_instruments, output_folder, n_clips)\n",
    "\n",
    "output_folder = os.path.join(processed_path, 'spectrograms')\n",
    "mixed_clips_df = generate_spectrograms(mixed_clips_df, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T20:43:25.252327Z",
     "start_time": "2024-04-15T20:43:25.245651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>spectrogram_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_0.wav</td>\n",
       "      <td>Hi-hat, Gong, Gong, Gong</td>\n",
       "      <td>electronic</td>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_0_spectrogram.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_1.wav</td>\n",
       "      <td>Double_bass, Snare_drum, Trumpet, Clarinet, Double_bass, Double_bass</td>\n",
       "      <td>jazz</td>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_1_spectrogram.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_2.wav</td>\n",
       "      <td>Snare_drum, Acoustic_guitar, Electric_piano, Acoustic_guitar, Bass_drum, Electric_piano</td>\n",
       "      <td>rock</td>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_2_spectrogram.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_3.wav</td>\n",
       "      <td>Gong, Gong, Chime, Gong, Finger_snapping, Gong, Bass_drum</td>\n",
       "      <td>electronic</td>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_3_spectrogram.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_4.wav</td>\n",
       "      <td>Snare_drum, Acoustic_guitar, Saxophone, Electric_piano, Electric_piano</td>\n",
       "      <td>blues</td>\n",
       "      <td>C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_4_spectrogram.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                       path  \\\n",
       "0  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_0.wav   \n",
       "1  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_1.wav   \n",
       "2  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_2.wav   \n",
       "3  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_3.wav   \n",
       "4  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\mixed_clips/mixed_clip_4.wav   \n",
       "\n",
       "                                                                                    labels  \\\n",
       "0                                                                 Hi-hat, Gong, Gong, Gong   \n",
       "1                     Double_bass, Snare_drum, Trumpet, Clarinet, Double_bass, Double_bass   \n",
       "2  Snare_drum, Acoustic_guitar, Electric_piano, Acoustic_guitar, Bass_drum, Electric_piano   \n",
       "3                                Gong, Gong, Chime, Gong, Finger_snapping, Gong, Bass_drum   \n",
       "4                   Snare_drum, Acoustic_guitar, Saxophone, Electric_piano, Electric_piano   \n",
       "\n",
       "        genre  \\\n",
       "0  electronic   \n",
       "1        jazz   \n",
       "2        rock   \n",
       "3  electronic   \n",
       "4       blues   \n",
       "\n",
       "                                                                                        spectrogram_path  \n",
       "0  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_0_spectrogram.png  \n",
       "1  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_1_spectrogram.png  \n",
       "2  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_2_spectrogram.png  \n",
       "3  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_3_spectrogram.png  \n",
       "4  C:\\GIT\\DAT255-G11-audio_project\\notebooks\\..\\data\\processed\\spectrograms\\mixed_clip_4_spectrogram.png  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_clips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(r): \n",
    "    return r['labels'].split(', ')\n",
    "\n",
    "def get_x(r): \n",
    "    return r['spectrogram_path']\n",
    "\n",
    "# Create a DataBlock\n",
    "db = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n",
    "               splitter=RandomSplitter(seed=42),\n",
    "               get_x=get_x,\n",
    "               get_y=get_y,\n",
    "               item_tfms=Resize(224)\n",
    "               )\n",
    "\n",
    "dls = db.dataloaders(mixed_clips_df, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "learn = vision_learner(dls, resnet34, metrics=partial(accuracy_multi, thresh=0.5), loss_func=BCEWithLogitsLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = learn.lr_find()\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learn.recorder.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metric_values = [x[1] for x in learn.recorder.values]\n",
    "\n",
    "plt.plot(metric_values, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Metric Value by Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions and targets\n",
    "preds, targets = learn.get_preds()\n",
    "\n",
    "# Convert the predictions to binary using a threshold\n",
    "threshold = 0.5\n",
    "binary_preds = (preds > threshold).int()\n",
    "\n",
    "# Print the first few predictions and targets\n",
    "for i in range(5):\n",
    "    print(f\"Predictions: {binary_preds[i]}, Targets: {targets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dls.vocab\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions and targets\n",
    "preds, targets = learn.get_preds()\n",
    "\n",
    "# Convert the predictions to binary using a threshold\n",
    "threshold = 0.5\n",
    "binary_preds = (preds > threshold).int()\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "binary_preds_np = binary_preds.numpy()\n",
    "targets_np = targets.numpy()\n",
    "\n",
    "# Print the first 5 predictions and targets\n",
    "for i in range(5):\n",
    "    predicted_labels = [class_names[j] for j in range(len(class_names)) if binary_preds_np[i][j]==1]\n",
    "    actual_labels = [class_names[j] for j in range(len(class_names)) if targets_np[i][j]==1]\n",
    "    print(f\"Predicted labels: {predicted_labels}, Actual labels: {actual_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each label\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(targets_np, binary_preds_np)\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(precision))\n",
    "plt.plot(x, precision, marker='o', linestyle='-', color='b', label='Precision')\n",
    "plt.plot(x, recall, marker='o', linestyle='-', color='r', label='Recall')\n",
    "plt.plot(x, f1, marker='o', linestyle='-', color='g', label='F1 Score')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision, Recall, and F1 Score for Each Label')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
